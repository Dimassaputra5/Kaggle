{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3310,"sourceType":"datasetVersion","datasetId":1919}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport optuna\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this at the start of your Kaggle notebook\n# !pip install -U scikit-learn==1.2.2\n# !pip install -U imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport missingno as mso\nimport seaborn as sns\nimport warnings\nimport scipy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\n\nfrom scipy import stats\nfrom scipy.stats import pearsonr\nfrom scipy.stats import ttest_ind\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = '/kaggle/input/chocolate-bar-ratings/flavors_of_cacao.csv'\ncc = pd.read_csv(data_path)\ncc","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfor column in cc.columns:\n    cc.columns = cc.columns.str.replace(r'\\s*\\n\\s*', ' ', regex=True)\ncc.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc.rename(columns={\n    'Company (Maker-if known)': 'Company',\n    'Specific Bean Origin or Bar Name': 'BarName',\n    'Review Date':'ReviewDate',\n    'Cocoa Percent':'CocoaPercent',\n    'Company Location':'CompanyLocation',\n    'Bean Type':'BeanType',\n    'Broad Bean Origin' : 'BroadBeanOrigin',\n}, inplace=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc.isnull().sum()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc['CocoaPercent'] = cc['CocoaPercent'].str.replace('%', '').astype(float)\ncc['CocoaPercent'].info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc['BeanType'] = (cc['BeanType']\n                 .str.strip()  # Menghapus leading/trailing spaces\n                 .replace('', np.nan))  # Mengganti string kosong dengan NaN\ncc['BeanType'] = cc['BeanType'].replace(np.nan, cc['BeanType'].mode()[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc['ReviewDate'] = pd.to_datetime(cc['ReviewDate'].astype(str), format='%Y', errors='coerce')\ncc['ReviewDate'] = cc['ReviewDate'].dt.year","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc.isnull().sum()\ncc['BroadBeanOrigin'] = cc['BroadBeanOrigin'].fillna(cc['BroadBeanOrigin'].mode()[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc_numerik = cc.select_dtypes(include=np.number)\ncc_numerik.info()\ncc_object = cc.select_dtypes(include='object')\ncc_object.info()\n\n# cc = pd.get_dummies(cc, columns=['Company','BarName','CompanyLocation','BeanType'], drop_first=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# le = LabelEncoder()\n# categorical_features = ['BeanType', 'BroadBeanOrigin', 'CompanyLocation']\n# for columns in categorical_features:\n#     cc[columns] = le.fit_transform(cc[columns])\n\nlabel_encoders = {}\ncategorical_features = ['BeanType', 'BroadBeanOrigin', 'CompanyLocation']\n\nfor column in categorical_features:  \n    label_encoders[column] = LabelEncoder()\n    cc[column] = label_encoders[column].fit_transform(cc[column])\n    \ncompany_freq = cc['Company'].value_counts(normalize=True)\ncc['Company'] = cc['Company'].map(company_freq)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"X = cc.drop(columns='BroadBeanOrigin')\ny = cc['BroadBeanOrigin']","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# 1. Check class distribution\nprint(\"Original class distribution:\")\nprint(y.value_counts())\n\n# 2. Filter classes with too few samples\nmin_samples = 6\nvalue_counts = y.value_counts()\nvalid_classes = value_counts[value_counts >= min_samples].index\nmask = y.isin(valid_classes)\nX_filtered = X[mask]\ny_filtered = y[mask]\n\nprint(\"\\nClass distribution after filtering:\")\nprint(y_filtered.value_counts())\n\n# 3. Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_filtered, y_filtered)\n\n# 4. Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, \n    y_resampled, \n    test_size=0.2, \n    random_state=42\n)\n\nprint(\"\\nFinal class distribution:\")\nprint(pd.Series(y_resampled).value_counts())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n\n\n# smote = SMOTE(random_state=42)\n# X_resampled, y_resampled = smote.fit_resample(X, y)\n# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LRclassifier = LogisticRegression(solver='saga', random_state=42)\nLRclassifier.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = LRclassifier.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\nfrom sklearn.metrics import accuracy_score\nLRAcc = accuracy_score(y_pred,y_test)\nprint('LR accuracy: {:.2f}%'.format(LRAcc*100))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    precision = tp / (tp + fp + 1e-8)\n    recall = tp / (tp + fn + 1e-8)\n\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n    return f1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# def objective(trial):\n#     n_estimators = trial.suggest_int('n_estimators', 10, 200)\n#     max_depth = trial.suggest_int('max_depth', 3, 20)\n#     min_samples_split= trial.suggest_int('min_samples_split', 2, 20)\n    \n#     model = RandomForestClassifier(\n#         n_estimators=n_estimators,\n#         max_depth=max_depth,\n#         random_state=42,\n#         bootstrap=True,\n#         min_samples_split=min_samples_split,\n#         max_features='sqrt'\n#     )\n    \n#     model.fit(X_train, y_train)\n\n#     y_pred = model.predict(X_test)\n\n#     score = f1_score(y_test, y_pred)\n\n#     return score\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials = 20)\n\n# best_params = study.best_params\n# print(\"Best Parameter:\", best_params)\n\n# best_model = RandomForestClassifier(\n#     n_estimators = best_params['n_estimators'],\n#     max_depth = best_params['max_depth'], \n#     random_state=42,\n#     bootstrap=True,\n#     min_samples_split=5,\n#     max_features='sqrt',    \n# )\n\n# best_model.fit(X_train, y_train)\n\n# y_pred = best_model.predict(X_test)\n\n# final_f1 = f1_score(y_test,y_pred)\n# print(\"Final f1:\", final_f1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T14:53:49.108Z"}},"outputs":[],"execution_count":null}]}